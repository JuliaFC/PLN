{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information Extraction no wiki movies.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XNJRcmoIJQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd2bc77-b44d-4e28-89f5-95f3ae6e9cb3"
      },
      "source": [
        "#http://alexminnaar.com/2019/08/22/ner-rnns-tensorflow.html\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZcIy9pmJSeH"
      },
      "source": [
        "df = pd.read_csv(\"entities_final.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "zavYxbNuKkWZ",
        "outputId": "8a000a09-091b-4e5b-c2cf-8026ca87a446"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>who's the writer of Experiment Perilous</td>\n",
              "      <td>['0', '0', '0', '0', 'B-movie', 'I-movie']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>who's the writer of Games</td>\n",
              "      <td>['0', '0', '0', '0', 'B-movie']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>which person wrote The Wolf Man</td>\n",
              "      <td>['0', '0', '0', 'B-movie', 'I-movie', 'I-movie']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>who is the writer of the film Sweet Charity</td>\n",
              "      <td>['0', '0', '0', '0', '0', '0', '0', 'B-movie',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>who was the writer of Extract</td>\n",
              "      <td>['0', '0', '0', '0', '0', 'B-movie']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      question                                           entities\n",
              "0      who's the writer of Experiment Perilous         ['0', '0', '0', '0', 'B-movie', 'I-movie']\n",
              "1                    who's the writer of Games                    ['0', '0', '0', '0', 'B-movie']\n",
              "2              which person wrote The Wolf Man   ['0', '0', '0', 'B-movie', 'I-movie', 'I-movie']\n",
              "3  who is the writer of the film Sweet Charity  ['0', '0', '0', '0', '0', '0', '0', 'B-movie',...\n",
              "4                who was the writer of Extract               ['0', '0', '0', '0', '0', 'B-movie']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKM_eugwZxrz"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "         df[\"question\"], df[\"entities\"], test_size=0.5, random_state=2021)\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "         X_test, y_test, test_size=0.5, random_state=2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM6vqyLdJXnt"
      },
      "source": [
        "labels = set([label for line in df[\"entities\"] for label in eval(line)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0iEvoNrb4fb"
      },
      "source": [
        "all_text = \" \".join([\" \".join(question) for question in df[\"question\"]])\n",
        "vocab = sorted(set(all_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of8tTTetJFAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e7166d-3cb3-4e2d-c541-304802a97050"
      },
      "source": [
        "# create character/id and label/id mapping\n",
        "char2idx = {u:i+1 for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "label2idx = {u:i+1 for i, u in enumerate(labels)}\n",
        "idx2label = np.array(labels)\n",
        "\n",
        "print(idx2label)\n",
        "print(char2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'I-actor', 'I-director', 'I-movie', 'I-writer', 'I-tag', 'B-writer', 'B-actor', 'B-tag', 'B-director', 'B-movie', '0'}\n",
            "{' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '+': 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, 'A': 21, 'B': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'J': 30, 'K': 31, 'L': 32, 'M': 33, 'N': 34, 'O': 35, 'P': 36, 'Q': 37, 'R': 38, 'S': 39, 'T': 40, 'U': 41, 'V': 42, 'W': 43, 'X': 44, 'Y': 45, 'Z': 46, 'a': 47, 'b': 48, 'c': 49, 'd': 50, 'e': 51, 'f': 52, 'g': 53, 'h': 54, 'i': 55, 'j': 56, 'k': 57, 'l': 58, 'm': 59, 'n': 60, 'o': 61, 'p': 62, 'q': 63, 'r': 64, 's': 65, 't': 66, 'u': 67, 'v': 68, 'w': 69, 'x': 70, 'y': 71, 'z': 72, '¡': 73, '½': 74, 'À': 75, 'É': 76, 'Õ': 77, 'à': 78, 'á': 79, 'â': 80, 'ã': 81, 'ä': 82, 'å': 83, 'æ': 84, 'ç': 85, 'è': 86, 'é': 87, 'ê': 88, 'ì': 89, 'í': 90, 'î': 91, 'ï': 92, 'ñ': 93, 'ò': 94, 'ó': 95, 'ô': 96, 'ö': 97, 'ø': 98, 'û': 99, 'ü': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHtLQrq4JhJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b8a33b-8215-4092-c436-ce898b0669a7"
      },
      "source": [
        "def split_char_labels(X, y):\n",
        "    '''\n",
        "    For a given input/output example, break tokens into characters while keeping \n",
        "    the same label.\n",
        "    '''\n",
        "\n",
        "    tokens = X\n",
        "    labels = eval(y)\n",
        "\n",
        "    input_chars = []\n",
        "    output_char_labels = []\n",
        "\n",
        "    for token,label in zip(tokens,labels):\n",
        "\n",
        "        input_chars.extend([char for char in token])\n",
        "        input_chars.extend(' ')\n",
        "        output_char_labels.extend([label]*len(token))\n",
        "        output_char_labels.extend('0')\n",
        "\n",
        "    return [[char2idx[x] for x in input_chars[:-1]],np.array([label2idx[x] for x in output_char_labels[:-1]])]\n",
        "\n",
        "train_formatted = list(map(split_char_labels, X_train, y_train))\n",
        "test_formatted = list(map(split_char_labels, X_test, y_test))\n",
        "valid_formatted = list(map(split_char_labels, X_val, y_val))\n",
        "\n",
        "print(len(train_formatted))\n",
        "print(len(test_formatted))\n",
        "print(len(valid_formatted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4999\n",
            "2500\n",
            "2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R5Y7eqPkWUC",
        "outputId": "54cccdc2-527e-41b0-ae2a-274cde7d540d"
      },
      "source": [
        "len(train_formatted[1][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDRSAIObKBL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26384953-73d3-4be2-d49f-33e2df6921c1"
      },
      "source": [
        "# training generator\n",
        "def gen_train_series():\n",
        "    for eg in train_formatted:\n",
        "        yield eg[0],eg[1]\n",
        "    \n",
        "    # validation generator\n",
        "def gen_valid_series():\n",
        "\n",
        "    for eg in valid_formatted:\n",
        "        yield eg[0],eg[1]\n",
        "    \n",
        "# test generator\n",
        "def gen_test_series():\n",
        "\n",
        "    for eg in test_formatted:\n",
        "        yield eg[0],eg[1]\n",
        "      \n",
        "# create Dataset objects for train, test and validation sets  \n",
        "series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "series_valid = tf.data.Dataset.from_generator(gen_valid_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "series_test = tf.data.Dataset.from_generator(gen_test_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE=1000\n",
        "    \n",
        "# create padded batch series objects for train, test and validation sets\n",
        "ds_series_batch = series.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "ds_series_batch_valid = series_valid.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "ds_series_batch_test = series_test.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "\n",
        "# print example batches\n",
        "for input_example_batch, target_example_batch in ds_series_batch_valid.take(1):\n",
        "    print(input_example_batch)\n",
        "    print(target_example_batch)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[69  1 54 ...  0  0  0]\n",
            " [69  1 54 ...  0  0  0]\n",
            " [69  1 54 ...  0  0  0]\n",
            " ...\n",
            " [69  1 54 ...  0  0  0]\n",
            " [69  1 54 ...  0  0  0]\n",
            " [69  1 54 ...  0  0  0]], shape=(128, 27), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[11 11 11 ...  0  0  0]\n",
            " [11 11 11 ...  0  0  0]\n",
            " [11 11 11 ...  0  0  0]\n",
            " ...\n",
            " [11 11 11 ...  0  0  0]\n",
            " [11 11 11 ...  0  0  0]\n",
            " [11 11 11 ...  0  0  0]], shape=(128, 27), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4msLXztSJtqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d75ed42-9b34-4847-96e5-3855c1c494af"
      },
      "source": [
        "  vocab_size = len(vocab)+1\n",
        "\n",
        "  # The embedding dimension\n",
        "  embedding_dim = 256\n",
        "\n",
        "  # Number of RNN units\n",
        "  rnn_units = 1024\n",
        "\n",
        "  label_size = len(labels)  \n",
        "  \n",
        "  # build LSTM model\n",
        "  def build_model(vocab_size,label_size, embedding_dim, rnn_units, batch_size):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None],mask_zero=True),\n",
        "            tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True, # a saida da lstm vai ser um conj de vetores\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "            tf.keras.layers.Dense(label_size)\n",
        "            ])\n",
        "        return model\n",
        "\n",
        "  model = build_model(\n",
        "        vocab_size = len(vocab)+1,\n",
        "        label_size=len(labels)+1,\n",
        "        embedding_dim=embedding_dim,\n",
        "        rnn_units=rnn_units,\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "  model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 256)          25856     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (128, None, 1024)         5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 12)           12300     \n",
            "=================================================================\n",
            "Total params: 5,285,132\n",
            "Trainable params: 5,285,132\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vnxVFcK1Hk"
      },
      "source": [
        "import os\n",
        "\n",
        "# define loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CQ2I9UDK9ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5222868f-756e-4ad0-993f-3e69ab152ae8"
      },
      "source": [
        "EPOCHS=20\n",
        "history = model.fit(ds_series_batch, epochs=EPOCHS, validation_data=ds_series_batch_valid,callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "39/39 [==============================] - 27s 616ms/step - loss: 0.4983 - sparse_categorical_accuracy: 0.7887 - val_loss: 0.3418 - val_sparse_categorical_accuracy: 0.8234\n",
            "Epoch 2/20\n",
            "39/39 [==============================] - 23s 576ms/step - loss: 0.3281 - sparse_categorical_accuracy: 0.8228 - val_loss: 0.2751 - val_sparse_categorical_accuracy: 0.8367\n",
            "Epoch 3/20\n",
            "39/39 [==============================] - 23s 583ms/step - loss: 0.2705 - sparse_categorical_accuracy: 0.8395 - val_loss: 0.2423 - val_sparse_categorical_accuracy: 0.8506\n",
            "Epoch 4/20\n",
            "39/39 [==============================] - 23s 583ms/step - loss: 0.2444 - sparse_categorical_accuracy: 0.8480 - val_loss: 0.2210 - val_sparse_categorical_accuracy: 0.8536\n",
            "Epoch 5/20\n",
            "39/39 [==============================] - 23s 594ms/step - loss: 0.2214 - sparse_categorical_accuracy: 0.8515 - val_loss: 0.2081 - val_sparse_categorical_accuracy: 0.8594\n",
            "Epoch 6/20\n",
            "39/39 [==============================] - 23s 591ms/step - loss: 0.2109 - sparse_categorical_accuracy: 0.8555 - val_loss: 0.1984 - val_sparse_categorical_accuracy: 0.8645\n",
            "Epoch 7/20\n",
            "39/39 [==============================] - 23s 582ms/step - loss: 0.1987 - sparse_categorical_accuracy: 0.8621 - val_loss: 0.1905 - val_sparse_categorical_accuracy: 0.8684\n",
            "Epoch 8/20\n",
            "39/39 [==============================] - 23s 596ms/step - loss: 0.1908 - sparse_categorical_accuracy: 0.8683 - val_loss: 0.1813 - val_sparse_categorical_accuracy: 0.8730\n",
            "Epoch 9/20\n",
            "39/39 [==============================] - 23s 591ms/step - loss: 0.1820 - sparse_categorical_accuracy: 0.8759 - val_loss: 0.1791 - val_sparse_categorical_accuracy: 0.8795\n",
            "Epoch 10/20\n",
            "39/39 [==============================] - 23s 581ms/step - loss: 0.1799 - sparse_categorical_accuracy: 0.8784 - val_loss: 0.1711 - val_sparse_categorical_accuracy: 0.8794\n",
            "Epoch 11/20\n",
            "39/39 [==============================] - 23s 593ms/step - loss: 0.1750 - sparse_categorical_accuracy: 0.8802 - val_loss: 0.1701 - val_sparse_categorical_accuracy: 0.8838\n",
            "Epoch 12/20\n",
            "39/39 [==============================] - 23s 593ms/step - loss: 0.1711 - sparse_categorical_accuracy: 0.8825 - val_loss: 0.1667 - val_sparse_categorical_accuracy: 0.8834\n",
            "Epoch 13/20\n",
            "39/39 [==============================] - 23s 584ms/step - loss: 0.1702 - sparse_categorical_accuracy: 0.8835 - val_loss: 0.1633 - val_sparse_categorical_accuracy: 0.8860\n",
            "Epoch 14/20\n",
            "39/39 [==============================] - 23s 585ms/step - loss: 0.1659 - sparse_categorical_accuracy: 0.8839 - val_loss: 0.1606 - val_sparse_categorical_accuracy: 0.8855\n",
            "Epoch 15/20\n",
            "39/39 [==============================] - 23s 594ms/step - loss: 0.1637 - sparse_categorical_accuracy: 0.8844 - val_loss: 0.1611 - val_sparse_categorical_accuracy: 0.8859\n",
            "Epoch 16/20\n",
            "39/39 [==============================] - 23s 588ms/step - loss: 0.1658 - sparse_categorical_accuracy: 0.8848 - val_loss: 0.1598 - val_sparse_categorical_accuracy: 0.8864\n",
            "Epoch 17/20\n",
            "39/39 [==============================] - 24s 616ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.8850 - val_loss: 0.1593 - val_sparse_categorical_accuracy: 0.8850\n",
            "Epoch 18/20\n",
            "39/39 [==============================] - 23s 597ms/step - loss: 0.1609 - sparse_categorical_accuracy: 0.8863 - val_loss: 0.1603 - val_sparse_categorical_accuracy: 0.8860\n",
            "Epoch 19/20\n",
            "39/39 [==============================] - 24s 602ms/step - loss: 0.1614 - sparse_categorical_accuracy: 0.8840 - val_loss: 0.1571 - val_sparse_categorical_accuracy: 0.8875\n",
            "Epoch 20/20\n",
            "39/39 [==============================] - 23s 596ms/step - loss: 0.1579 - sparse_categorical_accuracy: 0.8863 - val_loss: 0.1572 - val_sparse_categorical_accuracy: 0.8860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Oc6PpNmj8nR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a24eb3-3c2f-4e59-a4cb-d555b0802437"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "preds = np.array([])\n",
        "y_trues= np.array([])\n",
        "\n",
        "# iterate through test set, make predictions based on trained model\n",
        "for input_example_batch, target_example_batch in ds_series_batch_test:\n",
        "\n",
        "  pred=model.predict_on_batch(input_example_batch)\n",
        "  pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()\n",
        "  y_true=target_example_batch.numpy().flatten()\n",
        "\n",
        "  preds=np.concatenate([preds,pred_max])\n",
        "  y_trues=np.concatenate([y_trues,y_true])\n",
        "\n",
        "# remove padding from evaluation\n",
        "remove_padding = [(p,y) for p,y in zip(preds,y_trues) if y!=0]\n",
        "\n",
        "r_p = [x[0] for x in remove_padding]\n",
        "r_t = [x[1] for x in remove_padding]\n",
        "\n",
        "# print confusion matrix and classification report\n",
        "print(confusion_matrix(r_p,r_t))\n",
        "print(classification_report(r_p,r_t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    2     2     0     0     0     0     0     0     0     0     1]\n",
            " [   19    38     9     0     0     0     0     0     0     0    10]\n",
            " [    2    14  1965    40     0     0     0     0     0   286   377]\n",
            " [    0    36    56    91     0     0     0    16     0    27    57]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     1     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0    18     0    31    10     6]\n",
            " [    0     0   159     0     0     0     8     0    14   629   169]\n",
            " [  222    46   764   125    73   224   207    82    80   763 27657]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.01      0.40      0.02         5\n",
            "         2.0       0.28      0.50      0.36        76\n",
            "         3.0       0.67      0.73      0.70      2684\n",
            "         4.0       0.36      0.32      0.34       283\n",
            "         5.0       0.00      0.00      0.00         0\n",
            "         6.0       0.00      0.00      0.00         0\n",
            "         7.0       0.00      1.00      0.01         1\n",
            "         8.0       0.00      0.00      0.00         0\n",
            "         9.0       0.25      0.48      0.33        65\n",
            "        10.0       0.37      0.64      0.47       979\n",
            "        11.0       0.98      0.91      0.95     30243\n",
            "\n",
            "    accuracy                           0.89     34336\n",
            "   macro avg       0.26      0.45      0.29     34336\n",
            "weighted avg       0.93      0.89      0.90     34336\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JbDdzic2c5G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}